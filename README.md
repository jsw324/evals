# ğŸ¤– AI Agent Evaluation System

<div align="center">
    <img src="https://raw.githubusercontent.com/agentuity/cli/refs/heads/main/.github/Agentuity.png" alt="Agentuity" width="100"/> <br/>
    <strong>Multi-Agent Evaluation Framework</strong> <br/>
<br />
</div>

Welcome to the Agentuity AI Agent Evaluation System! This project provides a comprehensive framework for evaluating AI models using a multi-agent architecture that orchestrates dataset loading, prompt templating, model execution, and result analysis.

## ğŸ¯ Overview

This evaluation system uses multiple specialized agents to create a robust, scalable framework for testing AI models against ground truth datasets. Each agent has a discrete task and can pass information to others through the Agentuity key-value store.

### System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Dataset Loader â”‚â”€â”€â”€â–¶â”‚ Template Managerâ”‚â”€â”€â”€â–¶â”‚ Batch Processor â”‚
â”‚     Agent       â”‚    â”‚     Agent       â”‚    â”‚     Agent       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Report Generatorâ”‚â—€â”€â”€â”€â”‚ Metrics Calc.   â”‚â—€â”€â”€â”€â”‚ Evaluation      â”‚
â”‚     Agent       â”‚    â”‚     Agent       â”‚    â”‚ Runner Agent    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                        â”‚
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚ Response Comp.  â”‚â—€â”€â”€â”€â”‚ Model Interface â”‚
                       â”‚     Agent       â”‚    â”‚     Agent       â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ—ï¸ Agent Structure

### Core Agents

- **`dataset_loader`**: Loads and validates ground truth evaluation datasets
- **`template_manager`**: Handles prompt templates with variable substitution
- **`batch_processor`**: Orchestrates evaluation runs across multiple test cases
- **`evaluation_runner`**: Executes individual evaluation cases
- **`model_interface`**: Communicates with various AI models (Claude, GPT, etc.)
- **`response_comparator`**: Compares model outputs against expected results
- **`metrics_calculator`**: Computes accuracy, similarity scores, and other metrics
- **`report_generator`**: Produces comprehensive evaluation reports

## ğŸ“‹ Prerequisites

Before you begin, ensure you have the following installed:

- **Python**: Version 3.10 or higher
- **UV**: Version 0.5.25 or higher ([Documentation](https://docs.astral.sh/uv/))
- **Agentuity CLI**: Latest version

## ğŸš€ Getting Started

### Authentication

```bash
agentuity login
```

### Development Mode

Run the evaluation system in development mode:

```bash
agentuity dev
```

Or start locally without the console:

```bash
uv run server.py
```

## ğŸ“Š Usage

### 1. Prepare Your Dataset

Create a ground truth dataset in JSON format:

```json
[
  {
    "query": "What is the capital of New York?",
    "response": "Albany"
  },
  {
    "query": "What is the capital of California?", 
    "response": "Sacramento"
  }
]
```

### 2. Create an Evaluation Request

Send a request to the `batch_processor` agent:

```json
{
  "evaluation_id": "state_capitals_eval_001",
  "dataset": {
    "path": "datasets/state_capitals.json",
    "format": "query_response_pairs"
  },
  "prompt_template": {
    "template": "You are an expert on state capitals. Answer the following question: {{query}}",
    "variables": ["query"]
  },
  "model_config": {
    "model_name": "claude-3-5-sonnet-latest",
    "max_tokens": 100,
    "temperature": 0.1
  },
  "evaluation_settings": {
    "comparison_method": "exact_match",
    "batch_size": 10,
    "output_format": "detailed_report"
  }
}
```

### 3. Review Results

The system will generate a comprehensive evaluation report:

```json
{
  "evaluation_id": "state_capitals_eval_001",
  "summary": {
    "total_cases": 50,
    "accuracy": 0.94,
    "avg_response_time": 1.2
  },
  "detailed_results": [...],
  "model_config": {...}
}
```

## ğŸ”„ Data Flow

The evaluation system uses the Agentuity key-value store for agent communication:

1. **Initial Request** â†’ `batch_processor` agent
2. **Dataset Loading** â†’ Key: `eval_run_{id}_dataset`
3. **Template Processing** â†’ Key: `eval_run_{id}_template`
4. **Individual Cases** â†’ Key: `eval_run_{id}_case_{case_id}`
5. **Aggregated Results** â†’ Key: `eval_run_{id}_final`

## ğŸ› ï¸ Configuration

### Environment Variables

Set up API keys for different models:

```bash
agentuity env set --secret ANTHROPIC_API_KEY=your_key_here
agentuity env set --secret OPENAI_API_KEY=your_key_here
```

### Evaluation Settings

Configure default evaluation parameters:

```bash
agentuity env set DEFAULT_MODEL=claude-3-5-sonnet-latest
agentuity env set DEFAULT_BATCH_SIZE=10
agentuity env set DEFAULT_COMPARISON_METHOD=exact_match
```

## ğŸ“ Project Structure

```
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ dataset_loader/         # Loads evaluation datasets
â”‚   â”œâ”€â”€ template_manager/       # Manages prompt templates
â”‚   â”œâ”€â”€ batch_processor/        # Orchestrates evaluations
â”‚   â”œâ”€â”€ evaluation_runner/      # Runs individual cases
â”‚   â”œâ”€â”€ model_interface/        # Interfaces with AI models
â”‚   â”œâ”€â”€ response_comparator/    # Compares responses
â”‚   â”œâ”€â”€ metrics_calculator/     # Calculates metrics
â”‚   â””â”€â”€ report_generator/       # Generates reports
â”œâ”€â”€ datasets/                   # Ground truth datasets
â”œâ”€â”€ templates/                  # Prompt templates
â”œâ”€â”€ reports/                    # Generated evaluation reports
â”œâ”€â”€ .venv/                      # Virtual environment
â”œâ”€â”€ pyproject.toml             # Dependencies
â”œâ”€â”€ server.py                  # Server entry point
â””â”€â”€ agentuity.yaml            # Project configuration
```

## ğŸ¯ Comparison Methods

The system supports multiple comparison methods:

- **`exact_match`**: Exact string matching
- **`semantic_similarity`**: Vector similarity comparison
- **`regex_match`**: Pattern-based matching
- **`custom`**: User-defined comparison functions

## ğŸŒ Deployment

Deploy your evaluation system to Agentuity Cloud:

```bash
agentuity deploy
```

## ğŸ“– Advanced Features

### Custom Metrics

Extend the `metrics_calculator` agent to support custom evaluation metrics.

### Multi-Model Comparison

Run evaluations across multiple models simultaneously for comparative analysis.

### Streaming Evaluations

Process large datasets with streaming evaluation capabilities.

## ğŸ†˜ Troubleshooting

If you encounter issues:

1. Check agent logs in the Agentuity Console
2. Verify your dataset format matches the expected schema
3. Ensure all required environment variables are set
4. Join our [Discord community](https://discord.com/invite/vtn3hgUfuc) for support

## ğŸ“š Documentation

- [Agentuity Python SDK](https://agentuity.dev/SDKs/python)
- [Agent Development Guide](https://agentuity.dev/guides/agents)
- [Evaluation Best Practices](https://agentuity.dev/guides/evaluation)

## ğŸ“ License

This project is licensed under the terms specified in the LICENSE file.
